{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b768a85d-30e0-4bf6-832b-dc53c4d4d8e4",
   "metadata": {},
   "source": [
    "## ‚ÄúThis project is a mini prototype inspired by Google‚Äôs Regression Language Model (RLM).\n",
    "It takes system configurations as input (CPU, RAM, Disk), converts them into numeric tensors, and predicts efficiency using a small feedforward neural network.\n",
    "The purpose is to understand the RLM concept: learning from input-output examples in a supervised way, so the model can predict numeric outcomes for new, unseen configurations.\n",
    "While Google‚Äôs RLM works with raw textual logs, huge datasets, and large LLMs, this is a toy version running on CPU, demonstrating the core idea of text-to-numeric regression in a simple, understandable way.‚Äù\n",
    "\n",
    "By: **Akhilesh Pant** (MCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82070c5-7336-47dd-b67e-d33ddfa5d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Loss: 1338.5206\n",
      "Epoch 100 - Loss: 6.2625\n",
      "Epoch 150 - Loss: 0.1286\n",
      "Epoch 200 - Loss: 0.0743\n",
      "\n",
      "Predictions:\n",
      "Config: {'CPU': 8, 'RAM': 32, 'DISK': 1000} -> Predicted Efficiency: 2.15\n",
      "Config: {'CPU': 16, 'RAM': 64, 'DISK': 2000} -> Predicted Efficiency: 4.47\n",
      "Config: {'CPU': 32, 'RAM': 128, 'DISK': 4000} -> Predicted Efficiency: 9.12\n",
      "Config: {'CPU': 24, 'RAM': 96, 'DISK': 3000} -> Predicted Efficiency: 6.80\n"
     ]
    }
   ],
   "source": [
    "# Simulate text-to-numeric regression using PyTorch\n",
    "# Works on CPU with numeric predictions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Example dataset\n",
    "# -----------------------------\n",
    "data = [\n",
    "    {\"config\": {\"CPU\": 8, \"RAM\": 32, \"DISK\": 1000}, \"efficiency\": 2.5},\n",
    "    {\"config\": {\"CPU\": 16, \"RAM\": 64, \"DISK\": 2000}, \"efficiency\": 4.7},\n",
    "    {\"config\": {\"CPU\": 32, \"RAM\": 128, \"DISK\": 4000}, \"efficiency\": 8.9},\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Convert configs to numeric tensor\n",
    "# -----------------------------\n",
    "def config_to_tensor(config):\n",
    "    # Flatten nested config into a numeric vector\n",
    "    return torch.tensor([config[\"CPU\"], config[\"RAM\"], config[\"DISK\"]], dtype=torch.float32)\n",
    "\n",
    "X = torch.stack([config_to_tensor(d[\"config\"]) for d in data])\n",
    "y = torch.tensor([d[\"efficiency\"] for d in data], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Simple regression model\n",
    "# -----------------------------\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "model = SimpleRegressor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Training loop\n",
    "# -----------------------------\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Predict new configs\n",
    "# -----------------------------\n",
    "test_configs = [\n",
    "    {\"CPU\": 8, \"RAM\": 32, \"DISK\": 1000},\n",
    "    {\"CPU\": 16, \"RAM\": 64, \"DISK\": 2000},\n",
    "    {\"CPU\": 32, \"RAM\": 128, \"DISK\": 4000},\n",
    "    {\"CPU\": 24, \"RAM\": 96, \"DISK\": 3000},  # new example\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for cfg in test_configs:\n",
    "    x_tensor = config_to_tensor(cfg)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_tensor.unsqueeze(0)).item()\n",
    "    print(f\"Config: {cfg} -> Predicted Efficiency: {pred:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edf30b-ad33-402a-b5f2-35163ad088dd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîπ **Step by step (Code Explanation):**\n",
    "\n",
    "1. **You give example data** ‚Üí\n",
    "   Some computer system configs (`CPU, RAM, DISK`) and their **efficiency scores**.\n",
    "   Example:\n",
    "\n",
    "   * `{CPU: 8, RAM: 32, DISK: 1000} ‚Üí efficiency: 2.5`\n",
    "\n",
    "---\n",
    "\n",
    "2. **The code learns the pattern** ‚Üí\n",
    "   It converts configs into numbers (a tensor) and then trains a **small neural network** (regression model).\n",
    "\n",
    "   * The network tries to understand the relation:\n",
    "     **‚ÄúMore CPU / RAM / DISK ‚Üí higher efficiency.‚Äù**\n",
    "\n",
    "---\n",
    "\n",
    "3. **Training happens** ‚Üí\n",
    "\n",
    "   * The network guesses an efficiency.\n",
    "   * It checks how wrong it is (loss).\n",
    "   * Then adjusts itself to be less wrong.\n",
    "   * This repeats for 200 steps (epochs).\n",
    "\n",
    "---\n",
    "\n",
    "4. **Now the model is trained** ‚Üí\n",
    "   It has \"learned\" the pattern between **config** and **efficiency** from the small dataset.\n",
    "\n",
    "---\n",
    "\n",
    "5. **You test new configs** ‚Üí\n",
    "   Example: `{CPU: 24, RAM: 96, DISK: 3000}`\n",
    "\n",
    "   * You don‚Äôt tell the efficiency.\n",
    "   * The model **predicts** the efficiency value for you.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ In one line:\n",
    "\n",
    "üëâ **This code trains a tiny AI model that learns how computer configs (CPU, RAM, DISK) affect efficiency, and then it predicts the efficiency of new configs.**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037fcbe-3dd5-411b-b2d3-caad6bf1c30c",
   "metadata": {},
   "source": [
    "## **Mini-RLM project code line by line** \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Importing libraries**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import json\n",
    "```\n",
    "\n",
    "* `torch` ‚Üí Core PyTorch library.\n",
    "* `torch.nn` ‚Üí Used to build neural network layers.\n",
    "* `torch.optim` ‚Üí Optimizers (SGD, Adam) for training.\n",
    "* `random` ‚Üí For shuffling/sampling configs.\n",
    "* `json` ‚Üí To serialize/deserialize configs as strings.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Synthetic dataset (random configs with scores)**\n",
    "\n",
    "```python\n",
    "def generate_synthetic_dataset(num_samples=5000):\n",
    "    dataset = []\n",
    "    for _ in range(num_samples):\n",
    "        config = {\n",
    "            \"lr\": round(random.uniform(0.0001, 0.1), 4),\n",
    "            \"batch_size\": random.choice([16, 32, 64, 128]),\n",
    "            \"layers\": random.randint(1, 10),\n",
    "            \"dropout\": round(random.uniform(0.0, 0.5), 2),\n",
    "        }\n",
    "        score = (\n",
    "            0.5 * (1/config[\"lr\"]) +\n",
    "            0.3 * config[\"batch_size\"] +\n",
    "            0.2 * config[\"layers\"] -\n",
    "            100 * config[\"dropout\"] +\n",
    "            random.gauss(0, 5)\n",
    "        )\n",
    "        dataset.append((json.dumps(config), score))\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "* We create **random hyperparameter configs** (like learning rate, batch size, etc.).\n",
    "* `score` is a fake ‚Äúperformance metric‚Äù depending on config.\n",
    "* `dataset` is a list of `(config_string, score)` tuples.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Character-level tokenizer**\n",
    "\n",
    "```python\n",
    "class CharTokenizer:\n",
    "    def __init__(self, dataset):\n",
    "        chars = set(\"\".join([cfg for cfg, _ in dataset]))\n",
    "        self.char2idx = {c:i+1 for i,c in enumerate(sorted(chars))}\n",
    "        self.char2idx[\"<pad>\"] = 0\n",
    "        self.idx2char = {i:c for c,i in self.char2idx.items()}\n",
    "        self.vocab_size = len(self.char2idx)\n",
    "\n",
    "    def encode(self, text, max_len):\n",
    "        x = [self.char2idx.get(c, 0) for c in text]  \n",
    "        if len(x) < max_len:\n",
    "            x += [0]*(max_len - len(x))  \n",
    "        return x[:max_len]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return \"\".join([self.idx2char.get(i, \"\") for i in indices])\n",
    "```\n",
    "\n",
    "* **Tokenizer converts text configs into numbers.**\n",
    "* `char2idx` ‚Üí mapping from char ‚Üí index.\n",
    "* `encode` ‚Üí converts string into fixed-length padded list of ints.\n",
    "* `decode` ‚Üí converts numbers back into chars.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. RLM model**\n",
    "\n",
    "```python\n",
    "class MiniRLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, (h, _) = self.rnn(x)\n",
    "        out = self.fc(h[-1])\n",
    "        return out.squeeze()\n",
    "```\n",
    "\n",
    "* **Embedding layer** ‚Üí turns indices into vectors.\n",
    "* **LSTM** ‚Üí processes sequence of chars.\n",
    "* **FC (Linear)** ‚Üí outputs a single number (predicted score).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Dataset preparation**\n",
    "\n",
    "```python\n",
    "dataset = generate_synthetic_dataset(2000)\n",
    "tokenizer = CharTokenizer(dataset)\n",
    "max_len = 50  \n",
    "\n",
    "X = [torch.tensor(tokenizer.encode(cfg, max_len)) for cfg,_ in dataset]\n",
    "y = [torch.tensor([score], dtype=torch.float) for _,score in dataset]\n",
    "X, y = torch.stack(X), torch.stack(y)\n",
    "\n",
    "split = int(0.8*len(X))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "```\n",
    "\n",
    "* Convert configs to tensors (`X`).\n",
    "* Convert scores to float tensors (`y`).\n",
    "* Train-test split (80% train, 20% test).\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Model + optimizer + loss**\n",
    "\n",
    "```python\n",
    "model = MiniRLM(tokenizer.vocab_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "```\n",
    "\n",
    "* `Adam` optimizer.\n",
    "* `MSELoss` because it‚Äôs regression (predicting a numeric score).\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Training loop**\n",
    "\n",
    "```python\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "* Train for 5 epochs.\n",
    "* Forward pass ‚Üí Loss ‚Üí Backpropagation ‚Üí Update weights.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Evaluation**\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test)\n",
    "    test_loss = criterion(preds, y_test.squeeze())\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "* Switch to evaluation mode.\n",
    "* Calculate test loss (MSE).\n",
    "\n",
    "---\n",
    "\n",
    "## **9. New config predictions**\n",
    "\n",
    "```python\n",
    "new_configs = [\n",
    "    {\"lr\":0.001, \"batch_size\":32, \"layers\":5, \"dropout\":0.1},\n",
    "    {\"lr\":0.05, \"batch_size\":128, \"layers\":8, \"dropout\":0.2},\n",
    "]\n",
    "\n",
    "for cfg in new_configs:\n",
    "    text = json.dumps(cfg)\n",
    "    x_tensor = torch.tensor([tokenizer.encode(text, max_len)])\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_tensor).item()\n",
    "    print(f\"Config: {cfg}, Predicted Score: {pred:.2f}\")\n",
    "```\n",
    "\n",
    "* Encode **new configs**.\n",
    "* Predict scores with trained model.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d52e01-aeb7-466f-ab46-88a5ce6315e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Interview Questions with Answers on the Mini-RLM Project**\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Easy Level (Basics)\n",
    "\n",
    "**Q1. What is the purpose of the Mini-RLM project you built?**\n",
    "**A1.** The Mini-RLM project demonstrates how to build a simplified Recurrent/Residual Language Model (RLM-like) from scratch, using PyTorch. It handles text tokenization, synthetic dataset generation, model training, and predictions.\n",
    "\n",
    "**Q2. Why did we use a character-level tokenizer instead of a word-level tokenizer?**\n",
    "**A2.** A character-level tokenizer is easier to implement for small datasets and ensures every possible text input can be encoded without needing a large vocabulary.\n",
    "\n",
    "**Q3. What is the role of the synthetic dataset in this project?**\n",
    "**A3.** The synthetic dataset provides mock \"system configurations\" (CPU, RAM, OS) that the model can learn from, instead of relying on an external dataset.\n",
    "\n",
    "**Q4. Why do we pad or truncate sequences in the encode function?**\n",
    "**A4.** Padding ensures all sequences have the same length (`max_len`) so they can be batched and fed into the model consistently.\n",
    "\n",
    "**Q5. What does the `forward` method in the model do?**\n",
    "**A5.** It takes tokenized input, embeds it, applies transformer layers (self-attention + feed-forward), averages the sequence representation, and outputs a single prediction.\n",
    "\n",
    "**Q6. What activation function is used in the model?**\n",
    "**A6.** The Transformer layers internally use `ReLU` (for feed-forward blocks), and the final layer produces a single scalar output without activation (regression).\n",
    "\n",
    "**Q7. Why did we use `nn.TransformerEncoder`?**\n",
    "**A7.** Because it provides a ready-to-use transformer-based encoder with multi-head attention and residual connections, which are essential for learning sequence representations.\n",
    "\n",
    "**Q8. What is the optimizer used here?**\n",
    "**A8.** Adam optimizer is used for its adaptive learning rate and faster convergence.\n",
    "\n",
    "**Q9. Why did we use MSELoss as the loss function?**\n",
    "**A9.** Because the task is regression (predicting numeric \"system performance score\"), so Mean Squared Error is appropriate.\n",
    "\n",
    "**Q10. How do we test the model after training?**\n",
    "**A10.** We create new synthetic configurations, tokenize them, pass them through the model, and observe predicted performance scores.\n",
    "\n",
    "---\n",
    "\n",
    "## üü° Moderate Level (Applied Understanding)\n",
    "\n",
    "**Q11. What would happen if we didn‚Äôt pad sequences?**\n",
    "**A11.** Variable-length sequences would cause shape mismatch errors in batches, since PyTorch requires tensors of uniform shape for matrix operations.\n",
    "\n",
    "**Q12. How does self-attention help in this model?**\n",
    "**A12.** Self-attention allows the model to weigh relationships between different parts of the input sequence (like CPU and RAM) for better context-aware predictions.\n",
    "\n",
    "**Q13. What is the role of positional encoding in Transformers, and why might we need it here?**\n",
    "**A13.** Transformers don‚Äôt have recurrence, so positional encoding adds information about sequence order. In this mini model, it‚Äôs omitted for simplicity, but adding it would improve context handling.\n",
    "\n",
    "**Q14. Why did we average embeddings across sequence length before the final layer?**\n",
    "**A14.** Averaging provides a fixed-size representation summarizing the sequence, which can be used to predict the output.\n",
    "\n",
    "**Q15. Could we replace average pooling with `[CLS]` token representation?**\n",
    "**A15.** Yes. Adding a special token like `[CLS]` and using its final embedding is a common approach (used in BERT).\n",
    "\n",
    "**Q16. What is the effect of increasing the embedding dimension?**\n",
    "**A16.** Higher embedding dimension allows richer representations but increases computation and risk of overfitting on small datasets.\n",
    "\n",
    "**Q17. Why did we train only for 20 epochs in the example?**\n",
    "**A17.** Since it‚Äôs a synthetic, small dataset, longer training would cause overfitting without meaningful generalization.\n",
    "\n",
    "**Q18. How does tokenization affect the model‚Äôs ability to generalize?**\n",
    "**A18.** A poor tokenizer can miss unseen characters and cause `KeyError`. Expanding the vocabulary solves this issue.\n",
    "\n",
    "**Q19. What improvement was needed when we got `KeyError: '5'` earlier?**\n",
    "**A19.** We expanded the tokenizer vocabulary to include all digits and possible characters present in synthetic configs.\n",
    "\n",
    "**Q20. Can this model scale to real NLP tasks like translation? Why or why not?**\n",
    "**A20.** No, it‚Äôs too small. Real tasks need much deeper networks, larger vocabularies, positional encodings, and huge datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¥ Hard Level (Deep Concepts & Extensions)\n",
    "\n",
    "**Q21. How is this Mini-RLM different from a full-scale GPT model?**\n",
    "**A21.** GPT is autoregressive (predicts next token step-by-step) with large-scale pretraining, while Mini-RLM is a regression model trained from scratch with a tiny dataset.\n",
    "\n",
    "**Q22. What challenges would arise if we tried to scale this model to 1M configs?**\n",
    "**A22.** Memory issues, slower training, need for batching, and possible gradient instability would arise without optimizations like gradient clipping.\n",
    "\n",
    "**Q23. What is gradient clipping and why might we need it here?**\n",
    "**A23.** Gradient clipping prevents exploding gradients by capping values during backpropagation. Transformers often require it for stable training.\n",
    "\n",
    "**Q24. What is the computational complexity of self-attention?**\n",
    "**A24.** `O(n¬≤*d)` where `n` is sequence length and `d` is embedding size. This becomes expensive for long sequences.\n",
    "\n",
    "**Q25. How could we replace character-level tokenization with subword tokenization (BPE/WordPiece)?**\n",
    "**A25.** By segmenting text into frequent subwords instead of raw characters, reducing sequence length and improving efficiency.\n",
    "\n",
    "**Q26. Why might positional encoding improve performance in this project?**\n",
    "**A26.** Because order matters (e.g., \"RAM 16GB\" vs \"16GB RAM\"). Without position info, the model may treat them the same.\n",
    "\n",
    "**Q27. How would you modify this model to support classification instead of regression?**\n",
    "**A27.** Replace the final linear layer with one output per class and use `CrossEntropyLoss` instead of `MSELoss`.\n",
    "\n",
    "**Q28. Could we integrate a pre-trained embedding layer here?**\n",
    "**A28.** Yes, we could initialize the embedding layer with pre-trained embeddings (e.g., FastText or GPT embeddings) for better performance.\n",
    "\n",
    "**Q29. How would we evaluate this model beyond training loss?**\n",
    "**A29.** By splitting into train/test sets and using metrics like Mean Absolute Error (MAE) and R¬≤ Score.\n",
    "\n",
    "**Q30. Why does synthetic data sometimes lead to poor generalization?**\n",
    "**A30.** Because synthetic patterns may not reflect real-world complexity, leading to overfitting on toy distributions.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fingerprint)",
   "language": "python",
   "name": "fingerprint-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
